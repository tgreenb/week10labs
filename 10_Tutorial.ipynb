{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c853f1-c5ca-43ad-b32f-c6abef7708e1",
   "metadata": {},
   "source": [
    "UM MSBA - BGEN632\n",
    "\n",
    "# Week 10: Advanced Statistical Techniques in Python\n",
    "\n",
    "In last week's tutorial, we covered the basics of statistics in Python. The purpose of this tutorial to familiarize you with the Python implementation of a few statistical techniques associated with artificial intelligence systems.\n",
    "\n",
    "1. Data Reduction\n",
    "    1. Principal components analysis\n",
    "    2. Factor analysis\n",
    "    3. Cluster analysis\n",
    "1. Classification\n",
    "    1. Regression trees\n",
    "    2. Classification trees\n",
    "1. Neural Networks (separate optional tutorial)\n",
    "\n",
    "---\n",
    "\n",
    "Let's prepare out notebook before jumping into some math.\n",
    "\n",
    "## Notebook Setup\n",
    "\n",
    "### Load Modules\n",
    "\n",
    "Please remember to install modules before attempting to import them! While we used `sklearn` and `scipy` in Week 9, the `factor_analyzer` and `graphviz` are introduced in this week's tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3e490-e4cc-4a9d-9235-7f1a99eacf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for pca\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA as pca\n",
    "\n",
    "# for factor analysis\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "\n",
    "# for k-means clustering \n",
    "import sklearn.metrics as metcs\n",
    "from scipy.cluster import hierarchy as hier\n",
    "from sklearn import cluster as cls\n",
    "\n",
    "# for decision tree \n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from sklearn import tree\n",
    "\n",
    "# modules for plotting trees\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f94437-fed9-4bf4-ae8c-c27d89ed614f",
   "metadata": {},
   "source": [
    "### Set Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172fc25-7096-4982-b75f-44d15e7f8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(\"/Users/obn/Documents/GitHub/UM-BGEN632/week10labs/data\")  # change this to your filepath\n",
    "os.getcwd()  # confirm change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e2800-a5d7-4072-a8b8-7422e1894538",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab64105-3f0e-4353-86d4-7a866e3d31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load employee ratings dataset and subset variables of interest in new df\n",
    "ratings_df = pd.read_table('reduction_data_new.txt')\n",
    "ratings_df_pca = ratings_df[['peruse01', 'peruse02', 'peruse03', 'peruse04', 'peruse05', 'peruse06', \n",
    "                             'pereou01', 'pereou02', 'pereou03', 'pereou04', 'pereou05','pereou06', \n",
    "                             'intent01', 'intent02', 'intent03']]\n",
    "\n",
    "# load k-means example dataset\n",
    "kmeans_df = pd.read_table('kmeansdata.txt')\n",
    "\n",
    "# load taxon data\n",
    "taxon_df = pd.read_table('taxon.txt', sep=' ')\n",
    "taxon_complete_df = pd.read_csv('taxonomy.txt', sep='\\t')\n",
    "\n",
    "# load plant growth data\n",
    "pg_df = pd.read_table('pgfull.txt', sep='\\t')\n",
    "\n",
    "# load pollution data and subset variables of interest in new df\n",
    "pollute_df = pd.read_table('pollute.txt')\n",
    "pollute_df_trees = pollute_df[['Temp', 'Industry', 'Population', 'Wind', 'Rain', 'Wet.days']]\n",
    "\n",
    "# load titanic \n",
    "titanic_df = pd.read_csv('titanic_data.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ea7d4-ca4c-4ebe-a3c0-e7b4e0f7b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect data (remove hash marks at beginning of lines below and then run cell if you would like to see output)\n",
    "# alternatively, write your own code to inspect data using your preferred approach (e.g., head(), etc.)\n",
    "\n",
    "#print(\"\\033[1m Employee Ratings DataFrame for PCA and FA demo\\n \\033[0m\")\n",
    "#ratings_df_pca.info()\n",
    "\n",
    "#print(\"\\n\\033[1m Group DataFrame for Clustering demo\\n \\033[0m\")\n",
    "#kmeans_df.info()\n",
    "\n",
    "#print(\"\\n\\033[1m Taxon DataFrame for Clustering demo\\n \\033[0m\")\n",
    "#taxon_df.info()\n",
    "\n",
    "#print(\"\\n\\033[1m Full Taxon DataFrame for Clustering demo\\n \\033[0m\")\n",
    "#taxon_complete_df.info()\n",
    "\n",
    "#print(\"\\n\\033[1m Plant Growth DataFrame for Clustering demo\\n \\033[0m\")\n",
    "#pg_df.info()\n",
    "\n",
    "#print(\"\\n\\033[1m Pollution DataFrame for Classification demo\\n \\033[0m\")\n",
    "#pollute_df_trees.info()\n",
    "\n",
    "#print(\"\\n\\033[1m Titanic DataFrame for Classification demo\\n \\033[0m\")\n",
    "#titanic_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e1b52-cadf-4cf4-8c8d-e4f478fc3663",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Reduction\n",
    "\n",
    "A difficulty of any data-driven project is narrowing down the dataset. This is especially true when working with big data. While narrowing the scope of a project can help narrow the data, this often does not sufficiently reduce the data. Additionally, we may not be familiar enough with a dataset to choose the correct variables for a given analysis.\n",
    "\n",
    "We will go over two methodologies to reduce a dataset: reducing the number of columns and reducing the number of rows. \n",
    "\n",
    "The first methodology focuses on reducing the number of columns in a given dataset. *Note that these techniques are not designed to reduce the number of rows in a dataset*. Think of it in terms of reducing the number of variables you want to use. Additionally, these techniques are for structured, quantitative data. These techniques are principal components analysis (PCA) and factor analysis (FA).\n",
    "\n",
    "The second methodology reduces the number of rows in a dataset. We will focus on a single technique: cluster analysis. Why would we want to reduce the number of rows in a dataset? One reason is that we may not know which subgroup in the data should be the focus of analysis. By using clustering, we can focus on a specific group in a sample.\n",
    "\n",
    "All of these analyses fall under the umbrella term multivariate statistics. They deal with *multiple* variables. You might assume that because a technique falls under the umbrella of multivariate statistics that there will be target or dependent variables. Data reduction techniques typically do not have a target variable; we refer to them as *unsurpervised learning* techniques. We are not modeling for the sake of prediction or prescription as we did with regression in last week's module. The goal is not to understand how to predict some event or phenomenon. Instead, the goal in data reduction is to *understand what variables or sub-groups should be the focus of analysis* (e.g., in predictive modeling). In this way, data reduction techniques are tools for *exploratory data analysis*. \n",
    "\n",
    "### Principal Components Analysis (PCA)\n",
    "\n",
    "The first column-reducing technique we will focus on is principal components analysis, or PCA. PCA is a technique that creates linear combinations of your columns. In other words, it determines which columns are similar and lumps them together. This process leads to the creation of new columns. If you have 12 columns of data, for example, it may determine that in reality you have only 5 unique columns of data. While this is an oversimplification, it helps illustrate what PCA does.\n",
    "\n",
    "In most cases, the results of PCA are not used. While PCA creates brand new columns of data, it is very hard to interpret what those columns represent. For example, assume you have a dataset on smartphone sales and two of the variables include annual income and occupation. PCA combines these into a single column based on their similarity. What does this new column represent? Additionally, the data points for each of those columns is now transformed into a linear combination creating new data points for a new column. What are these new data points? Can you interpret them?\n",
    "\n",
    "The simple answer is \"no.\" The new columns from a PCA can be used effectively within certain types of analyses, but that usage is beyond the scope of this class. Why even talk about it then? While we will not use the newly created columns from PCA, the results from PCA can inform our decision making whenselecting columns of data (i.e., which columns are considered redundant). This process provides a methodology in finding the number of redundant columns of data and removing them. This process is fairly easy to perform once you have learned how to perform it. \n",
    "\n",
    "We will perform PCA in conjunction with factor analysis (FA). When performing a PCA followed by a FA you should split your data into two separate halves. You should never use the same sample of data for both. This is similar to creating subsamples for training, testing, and validation. Each one should have different variance from the other. \n",
    "\n",
    "When a PCA is performed on data, the result is the creation of components, equal in number to the number of variables you used as input. If you have 6 variables, you will end up with 6 components. Simply put, these components represent a conceptual variable that explains some of the variance in the data.\n",
    "\n",
    "What does this mean? If your 6 variables are different colored liquids in different cups, what would happen when you pour them into a single pitcher? You end up with a single color, one comprised of all the other colors. What determines the color of the liquid in the pitcher? First, the color of each individual liquid. If you have 3 yellow and 3 blue, you will end up with a fairly even green. What if you have 1 yellow and 5 blue? Not really a green. Second, the intensity of the color. If you have 1 intense yellow and 5 extremely pale blues, you can end up with a very green liquid.\n",
    "\n",
    "This is how PCA operates. Some variables are more \"intense in color\" while other variables are more influential because they act as a group and have greater influence than other variables. The components that PCA generates represent the extent to which the \"colors\" influence the final product.\n",
    "\n",
    "Two methods are utilized to illustrate the components:\n",
    "\n",
    "1. Scree Plot\n",
    "2. Eigenvalues\n",
    "\n",
    "#### Scree Plot\n",
    "\n",
    "A scree plot simply generates a chart of the component values. Scree plots are more subjective than assessing eigenvalues. The purpose of using the scree plot is to determine how many components, or columns of data, you are truly dealing with. The way to read a scree plot is determining where the plot levels off and becomes flat; anything prior to that leveling off is a component that remains. \n",
    "\n",
    "Run the code cells below to see an example scree plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255b491-cd09-4f4e-89d1-75d5e3b0b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the PCA - we specify 15 components because we have 15 variables in the DataFrame\n",
    "pca_result = pca(n_components=15).fit(ratings_df_pca)\n",
    "\n",
    "# generate scree plot - focus on the output\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15], pca_result.explained_variance_ratio_, '-o')\n",
    "plt.ylabel('Proportion of Variance Explained') \n",
    "plt.xlabel('Principal Component') \n",
    "plt.xlim(0.75,4.25) \n",
    "plt.ylim(0,1.05) \n",
    "plt.xticks([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a3d83b-9b00-4108-a2c3-55428bf91759",
   "metadata": {},
   "source": [
    "Scree plots typically have a right-tailed pattern to them. Why? This is because the components are listed (starting on the left) from those that contribute the most influence to those (on the right) that contribute the least amount. Typically, the scree plot exhibits a steep plunge followed by a \"leveling off\" of the components. The components in the flat plane are not considered very influential while those that are part of the steep plunge are the most influential. In other words, if these components each represent a colored liquid, the first three cups contribute the most to the final, overall color in the pitcher, while the other cups hardly contribute much.\n",
    "\n",
    "In the scree plot above, you can see that the first 3 components are not flat. Starting with `Component 4` you can see the plot is level and flat. This suggests that out of 15 variables, only 3 should be used for the analysis. This means that some combination of all 15 variables should yield just 3.\n",
    "\n",
    "#### Eigenvalues\n",
    "\n",
    "The eigenvalues are more numerical in nature. The eigenvalues are the square of the standard deviation, or the variance. As stated earlier, the purpose of this tutorial is not to go into PCA too deeply, so do not worry too much about why this value is used. What is important is how it is interpreted. A component with an eigenvalue greater than or equal to 1.0 is considered influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3d243-61cc-4c71-a9a3-69d7dbfd5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain eigenvalues - focus on the output\n",
    "pca_result.explained_variance_\n",
    "\n",
    "# components from the PCA\n",
    "# pca_result.components_.T * np.sqrt(pca_result.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70dd7e8-68f3-4f57-8c32-6f21362f6e51",
   "metadata": {},
   "source": [
    "Each component has an associated value. You will notice that only three components are larger than 1.0. The way this PCA is assessed is any value greater than 1.0 is retained, while anything less than 1.0 is thrown out. The first three components all have values greater than 1.0. Starting with `Component 4`, however, the values are all smaller than 1.0. What this assessment tells us, is that in reality we are only dealing with 3 variables. The results of the scree plot and eigenvalues are consistent with each other.\n",
    "\n",
    "We used a relevant metric, percentage of variance explained by the principal component, in the code for the scree plot. We can get and inspect these numbers like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efddb2-bcbe-4c5a-856c-0cf304ecf1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain percentage of variance explained by the principal component\n",
    "pca_result.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb9d39-1cb2-4b9d-a896-7f052de475a7",
   "metadata": {},
   "source": [
    "Note the first value which corresponds to principal component 1: 0.47407443. This indicates that `Component 1` is able to explain 47.41% variance in the data. The value was calculated by dividing the eigenvalue of the first principal component by the sum of all eigenvalues and multiplying by 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec87ec-dbd6-4f11-8695-1c553a50477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(9.24664807 / (9.24664807 + 3.782521 + 2.2623671 + 0.72252661 + 0.54177949 + \n",
    "               0.49927148 + 0.43482191 + 0.3789874 + 0.32486612 + 0.30119841 + \n",
    "               0.27545114 + 0.26150443 + 0.1796185 + 0.15909693 + 0.133975)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c489cf-122e-4682-9f11-a778cd6a0caa",
   "metadata": {},
   "source": [
    "We could repeat this calculation with the eigenvalue of each component to produce the values obtained in the `explained_variance_ratio_` output. \n",
    "\n",
    "When conducting this analysis, the results from the scree plot and the eigenvalues are compared; you should notice similar results between the two, though you may find that they differ by one component. Once you determine the number of components to use, this informs you how many variables you might want to consider for your data model. At this point, however, you still are unsure which specific variables to use and the PCA cannot yield any more information. We can perform a factor analysis to determine which variables will be removed (or possibly combined with others).\n",
    "\n",
    "#### Scaling Data \n",
    "\n",
    "As a quick aside, sometimes data need to be scaled prior to running a PCA. This will depend on the data. If the variables in a dataset are measured on different scales (e.g., one variable can take any value between 0 and 1 and another variable can take any value between 0 and 100), model performance will suffer. This is because variables on a larger scale will have a stronger effect than variables on a smaller scale. To improve model performance, we can apply a method for scaling data: standardization, normalization, and min-max scaling. These methods transform values so that they are within a specific range (e.g., 0 to 1).  We skipped this step in our code above because the data has the same scaling: all variables in `ratings_df_pca` are measured on the same scale. In this case, the variables are measured on a 7-point scale. However, for the sake of demonstration, here is a code example for scaling with the `ratings_df_pca` DataFrame:\n",
    "\n",
    "```Python\n",
    "# import modules\n",
    "from sklearn.decomposition import PCA as pca\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# load and subset data\n",
    "ratings_df = pd.read_table('reduction_data_new.txt')\n",
    "\n",
    "ratings_df_pca = ratings_df[['peruse01', 'peruse02', 'peruse03', 'peruse04', 'peruse05', 'peruse06', \n",
    "                             'pereou01', 'pereou02', 'pereou03', 'pereou04', 'pereou05','pereou06', \n",
    "                             'intent01', 'intent02', 'intent03']]\n",
    "\n",
    "# scale data using standardization\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaled_data = scaler.fit_transform(ratings_df_pca)\n",
    "\n",
    "# run PCA with scaled data\n",
    "pca_result = pca(n_components=15).fit(scaled_data)\n",
    "```\n",
    "\n",
    "### Factor Analysis (FA)\n",
    "\n",
    "Just like PCA, a factor analysis can help reduce the variables within a given dataset. Factor analysis works by grouping variables or columns of data by how similar they behave. That is, how well do they mimic each other? For example, assume you have three columns of data for the Griz football team: `US shoe size`, `EU shoe size`, and `hair color`. The factor analysis will determine that `US shoe size` and `EU shoe size` behave similarly. It will suggest to you that the measures of shoe size are really the same type of data. You have two potential choices to make: somehow merge the two columns of data or drop one and keep the other. Either way, you will have just two columns.\n",
    "\n",
    "A factor analysis can be performed using different types of *rotations*. Some rotations assume the data has no relationship and forces them to be *orthogonal* (i.e., not correlated), like a promax rotation. Others assume a relationship such as *varimax* (i.e., correlated). While this is an important concept to know, it is something that is beyond the scope of this class and no further detail will be given. This is mentioned here so that you are aware of its existence. For the sake of this module, only the varimax rotation will be used. In statistics, being conservative in your approach is always better, and assuming that everything is correlated is better than assuming that everything is not correlated.\n",
    "\n",
    "For this section of the tutorial we will primarily use functions `factor_analyzer` module. \n",
    "\n",
    "#### Data Screening\n",
    "\n",
    "To begin, we will first check the correlation matrix for our variables. Then we will assess the dataset with two tests: Bartlett’s test of sphericity and Kaiser-Meyer-Olkin (KMO) test.\n",
    "\n",
    "Factor analysis is based on the correlation matrix of the variables of interest. We check the correlation matrix to examine how well the variables relate to one another. We would like to see a variety of correlation coefficient values. Generally, we are concerned about:\n",
    "\n",
    "* Variables that have no relationship with any other variables (i.e., they do not correlate): $r\\lt .30$\n",
    "* Variables that correlate too highly with other variables: $r\\gt .90$\n",
    "  * We want to avoid extreme multicollinearity or singularity.\n",
    "  * Extreme *multicollinearity*: variables that are very highly correlated.\n",
    "  * *Singularity*: variables that are perfectly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ecdb4-087e-49e4-9f2c-14359da45382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "ratings_df_pca.corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d358a5c-c765-4bd1-ab30-23bc5959cb0a",
   "metadata": {},
   "source": [
    "Our correlation matrix has a blend of (not too) low and (not too) high values. Unsurprisingly, variables (items) from the same scale (e.g., intent items) have higher correlations with each other than variables from different scales (e.g., intent and peruse items). This makes sense because the items from the same scale are measuring the same construct.\n",
    "\n",
    "Next, we will employ Bartlett’s test of sphericity. This tests the null hypothesis that the correlation matrix is an [identity matrix](https://mathworld.wolfram.com/IdentityMatrix.html). \n",
    "\n",
    "* *Identity matrix*: a matrix in which all of the diagonal elements are 1 and all off diagonal elements are 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81b589-a591-4135-91d5-53f2efc5d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bartlett’s test of sphericity -\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(ratings_df_pca)\n",
    "chi_square_value, p_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eaa4aca-43d7-4c34-9839-f9f610781d9f",
   "metadata": {},
   "source": [
    "Recall that *p* < .05 is our threshold for significant. As you can see, the *p*-value for our test is 0.0; it is is statistically significant and we can reject the null hypothesis. Note that if it were not statistically significant, we cannot reject the null hypothesis and should not employ a factor analysis. \n",
    "\n",
    "Okay, now on to the Kaiser-Meyer-Olkin (KMO) test. The KMO test evaluates the distribution of values, specifically how much shared variance there is between pairs of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979dc7e0-4986-4eaa-bf00-9f18bbbd1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMO test\n",
    "kmo_all, kmo_model = calculate_kmo(ratings_df_pca)\n",
    "kmo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd748ccf-b235-4fe0-82e1-a0d2ad41be0b",
   "metadata": {},
   "source": [
    "KMO values range between 0.0 and 1.0, where a value less than 0.6 is considered inadequate. With a KMO of 0.88, we can proceed with the factor analysis.\n",
    "\n",
    "#### Factor Extraction\n",
    "\n",
    "Now we are ready to carry out the factor analysis. In the next code cell, we first choose the number of factors and specify the type of rotation that we will use in the analysis. Then, we fit the factor analysis model. Lastly, we get and inspect the factor loadings matrix. Note that you may see a deprecation warning when you run the code; don't worry about this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe73ec-96d2-4c8f-8aa8-ead05e4b8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of factors selected based on PCA results, choosing varimax rotation\n",
    "fa = FactorAnalyzer(n_factors = 3, rotation='varimax')\n",
    "\n",
    "# fit FA model\n",
    "fa.fit(ratings_df_pca)\n",
    "\n",
    "# get factor loadings\n",
    "factor_df = pd.DataFrame(fa.loadings_, index=ratings_df_pca.columns, columns=['Factor 1', 'Factor 2', 'Factor 3'])\n",
    "factor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca568c7-8f90-4179-bee8-02892f250d71",
   "metadata": {},
   "source": [
    "To evaluate if/how variables have loaded on to factors, we apply a threshold. There is some disagreement about how the factor loading cutoff should be determined. You can read a bit about that [here](https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/thresholds). Again, it is best to be conservative so we will apply a factor loading threshold of 0.6. Based on that, we can observe the following:\n",
    "\n",
    "* Factor 1 made up of all `peruse` items (perceived usefulness of system).\n",
    "* Factor 2 made up of the `pereou` items (perceived ease of use of system).\n",
    "* Factor 3 made up of all `intent` items (intent to use system).\n",
    "\n",
    "In this case, all items load cleanly on to one of three factors and they load consistently (e.g., all intent items load on to the same factor). If we had observed, for example, that only `intent01` and `intent02` loaded onto the third factor and that `intent03` did not load cleanly on to any factor, then it might be a candidate for removal. \n",
    "\n",
    "We could also evaluate our factor analysis based on the explained variance with the following code:\n",
    "\n",
    "```Python\n",
    "exp_var_df = pd.DataFrame(fa.get_factor_variance(), index=['Sums of Squared Loadings', 'Proportion of Variance', 'Cumulative Variance'], columns=['Factor 1', 'Factor 2', 'Factor 3'])\n",
    "exp_var_df  # the cumulative variance is nearly 70% which is acceptable\n",
    "```\n",
    "\n",
    "Lastly, we may be interested in deciding if the variables should be averaged within a factor (composite measure) or if it is preferable to select a representative variable for each factor (single-item measure). A composite measure is generally more reliable and has greater validity (in statistical terms). A single-item measure is generally easier to interpret. Some folks recommend selecting representative columns if item factor loadings are not strong ($\\gt 0.80$). I recommend that you also consider the complexity of the construct that is being measured and how the items relate to your goal(s) with the overall analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82926ec6-279b-45c0-87da-a5a6a0f6a6f3",
   "metadata": {},
   "source": [
    "### Cluster Analysis\n",
    "\n",
    "Clustering is an unsupervised data mining method in which items are grouped into classes of similar data points. Clustering algorithms segment records minimizing within-cluster variance. The algorithms maximize between-cluster variance. Keep in mind, clustering is an *exploratory* analysis technique. It is not a predictive model, but can help you build your model by understanding your data's attributes, characteristics, and nuances. As a comparison, it is similar to database segmentation where the tuples of similar records are listed together.\n",
    "\n",
    "The purpose of PCA and FA is to reduce the number of columns in a dataset. Cluster analysis allows you to determine grouping within your data. Cluster analysis does this by determining which variables are best for apportioning group membership. Like PCA and FA, cluster analysis is an unsupervised technique; there is no target variable. Many different types of clustering algorithms exist, such as k-means, agglomerative analysis, and divisive clustering.\n",
    "\n",
    "Cluster analysis attempts to maximize the between-cluster variance and minimizing within-cluster variance. k-means, k-medians, k-modes all are centroid-based algorithms. That is, each cluster is created based on a central point in space in which its data points surround. Hierarchical clustering, which includes agglomerative and divisive, are based on distance of objects to one another, not a centroid.\n",
    "\n",
    "<div><center>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*nm43wXkfUKrEjF9DLJcapA.png\" width=500>\n",
    "</center></div>\n",
    "\n",
    "The purpose of the algorithm is to ensure that these groups remain separate by maximizing the between-cluster variance (green arrow) while at the same time minimizing the within-cluster variation (blue arrows).\n",
    "\n",
    "#### k-means Clustering\n",
    "\n",
    "k-means is a centroid-based clustering technique. A few different types are available:\n",
    "\n",
    "* k-means: the most basic version, it is centroid-based on mean averaging effects.\n",
    "* k-medians: relies on the median and is not sensitive to outliers, unlike mean;\n",
    "* k-medoids: centroid is not an unrealistic option; uses an actual cluster member (i.e. data point) as centroid;\n",
    "* Fuzzy Clustering: membership is not based on hard boundaries; a data point can belong to more than 1 cluster;\n",
    "\n",
    "What does the `k` stand for? This defines the *number of clusters* used in the algorithm. Typically, no a priori reason exists for selecting a value of `k`. When using k-means clustering, you should try multiple values of `k`. Once you have identified the number of clusters, or `k`, then each data point in the dataset is assigned to a cluster. The convergence criteria is based on the squared error.\n",
    "\n",
    "The following is a simple example to illustrate how the centroid is determined. Consider a dataset with four data points in a three-dimensional (x, y, z) space:\n",
    "1. (1, 1, 1)\n",
    "2. (1, 2, 1)\n",
    "3. (1, 3, 1)\n",
    "4. (2, 1, 1) \n",
    "\n",
    "Averaging x, y, and z together for each point yields the centroid:\n",
    "* Average the `x` values: $(1 + 1 + 1 + 2)\\div{4} = 1.25$\n",
    "* Average the `y` values: $(1 + 2 + 3 + 1)\\div{4} = 1.75$\n",
    "* Average the `z` values: $(1 + 1 + 1 + 1)\\div{4} = 1.00$\n",
    "\n",
    "This results in the centroid o$(1.25, 1.75, 1.00)$. Note that this data point is not one of the actual values in the dataset. One of weaknesses of the k-means family is that *centroids tend to not be real values in the dataset*.\n",
    "\n",
    "To help you more fully understand how k-means operates, let's consider a more complex example. Here are the typical steps a k-means algorithm follows:\n",
    "\n",
    "1. Analyst specifies `k` = number of clusters to partition data\n",
    "2: Initial cluster centers are randomly assigned to data, creating `k` cluster centers\n",
    "3: For each row in the data, find the nearest cluster center; each cluster center \"owns\" a subset of records resulting in *k* clusters, `C1`, `C2`, ..., `Ck` \n",
    "4: For each of `k` clusters, find cluster centroid, then update cluster center location to centroid\n",
    "5: Repeat Steps 3 - 5 until convergence or termination\n",
    "\n",
    "The k-means algorithm terminates when the centroids, or center of clusters, no longer change.\n",
    "\n",
    "For this example, assume we have the following data:\n",
    "\n",
    "| Name | Value |\n",
    "|:---:|:---:|\n",
    "| a | (1, 3) |\n",
    "| b | (3, 3) |\n",
    "| c | (4, 3) |\n",
    "| d | (5, 3) |\n",
    "| e | (1, 2) |\n",
    "| f | (4, 2) |\n",
    "| g | (1, 1) |\n",
    "| h | (1, 2) |\n",
    "\n",
    "* *Step 1*: `k = 2` specifies the number of clusters to partition\n",
    "* *Step 2*: Randomly assign `k = 2` cluster centers\n",
    "  * Assume the program randomly selects `(1, 1)` and `(2, 1)` for cluster 1 (`C1`) and cluster 2 (`C2`), respectively\n",
    "* *Step 3*: For each record, find the Euclidean distance from points `C1` and `C2`.\n",
    "\n",
    "| Point | a | b | c | d | e | f | g | h |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| Distance from C1 | 2.00 | 2.83 | 3.61 | 4.47 | 1.00 | 3.16 | 0.00 | 1.00 |\n",
    "| Distance from C2 | 2.24 | 2.24 | 2.83 | 3.61 | 1.41 | 2.24 | 1.00 | 0.00 |\n",
    "| Membership | C1 | C2 | C2 | C2 | C1 | C2 | C1 | C2 |\n",
    "\n",
    "The results of this initial pass through the data results in the following:\n",
    "\n",
    "* `C1: a, e, g`\n",
    "* `C2: b, c, d, f, h`\n",
    "\n",
    "Next, the Sum of Square Error (SSE) is calculated from Euclidean distances:\n",
    "* $SSE = 2.00+2.24+2.83+3.61+1.00+2.24+0+0$\n",
    "* $SSE = 36.0$\n",
    "\n",
    "Recall that *between-cluster variation (BCV) is maximized* and *within-cluster variation (WCV) is minimized*. The smallest *SSE* through all iterations is desired; though, you may not find the optimal minimum. It depends on where the cluster centroids are seeded. Since it is typically randomly selected, each time you run the algorithm may yield different results.\n",
    "\n",
    "Moving on to the next step:\n",
    "* *Step 4*: For `k` clusters, find cluster centroid, update location\n",
    "  * $C1 = [((1 + 1 + 1)\\div{3}), ((3 + 2 + 1)\\div{3})] = (1, 2)$\n",
    "  * $C2 = [((3 + 4 + 5 + 4 + 2)\\div{5}), ((3 + 3 + 3 + 2 + 1)\\div{5})] = (3.6, 2.4)$\n",
    "\n",
    "At this point, the process iterates through steps 3 through 4 until convergence occurs. Convergence may occur when the cluster centroids are essentially static and do not change, the rows of data do not change cluster membership, or other stopping criteria is fulfilled such as time or number of iterations.\n",
    "\n",
    "k-means is not guaranteed to find global minimum *SSE*. Instead, the local minimum may be found (due to seeding). Invoking an algorithm using a variety of initial cluster centers improves probability of achieving a global minimum. One approach places the first cluster at random point, with the remaining clusters placed far from previous centers.\n",
    "\n",
    "A few caveats. k-means is unable to handle categorical data. In addition, basic k-means cannot handle outliers. Recall that extreme values have a strong effect on mean. Consider using k-medians if extreme outliers are present. If the dataset contains categorical data, then we can use k-modes. This family of clustering techniques is not time efficient and does not scale well. Additionally, business domain knowledge is required to choose $k$ and interpret the results.\n",
    "\n",
    "##### Cluster Analysis in Python\n",
    "\n",
    "We will use `kmeans_df` in this section of the tutorial. This dataset contains four variables, two of which are categorical: `group` and `grouping`. The variable `grouping` has values A, B, C, D, E, and F. These represent natural groupings within the data. When using clustering, we do not typically have variables such as `grouping` or `group` because we are unaware of how the data group together. We are merely using this data to illustrate how clustering works.\n",
    "\n",
    "k-means is based on the idea that you choose the number of clusters *k* and the algorithm will determine, based on initial seeding, where those clusters are in the data.\n",
    "\n",
    "In this demonstration, we will  convert our categorical data and then process the data using 4 and 6 clusters, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c560f-22cc-49b4-9306-2f8e129331fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to categorical datatype\n",
    "kmeans_df['group'] = kmeans_df['group'].astype('category')          \n",
    "kmeans_df['grouping'] = kmeans_df['grouping'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf3f17-660e-4476-b550-fb23129ebf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 4 clusters\n",
    "km_4 = cls.KMeans(n_clusters=4).fit(kmeans_df.loc[:, ['x', 'y']])\n",
    "\n",
    "# assigned clusters\n",
    "km_4.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5c0d1-70a4-4315-adaa-01bf8bcc2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 6 clusters\n",
    "km_6 = cls.KMeans(n_clusters=6).fit(kmeans_df.loc[:, ['x', 'y']])\n",
    "\n",
    "# assigned clusters\n",
    "km_6.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254781a-df0d-4013-b95d-4c9b07b6fa10",
   "metadata": {},
   "source": [
    "The classification of the data for both cluster algorithms performs fairly well. The results of the 4-cluster result `km_4.labels_` shows some consistency (pay attention to grouping of values). When the clustering is switched to the 6-cluster group `km_6.labels_`, the results appear to be improved.\n",
    "\n",
    "We can assess misclassification using a confusion matrix in Python. The next two images show two methods to accomplish this. The first is a text-based matrix that does not have labels (though, that is something you could easily change). The second figure presents a color-based gradient of the exact same data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55ed80-dadf-4cd8-a92f-fe3f4475d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix - numerical representation\n",
    "cm = metcs.confusion_matrix(kmeans_df.group, km_6.labels_)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee60c79-417d-4ff6-8a13-2207bbedb0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix - graphical representation\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Actual Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.xticks([0,1,2,3,4,5], ['A','B','C','D','E','F'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14c911-ecac-48ee-bbdb-c25a3095eeb9",
   "metadata": {},
   "source": [
    "In both the numerical and graphical representations of the confusion matrix, we can see that group `A` is perfectly assigned to cluster `6`. Group `B` was mostly placed in cluster `5` with one member in cluster `2` and two in cluster `4`. Groups `C` and `F` have the worst placement. Despite this, the clustering did a fairly good job with the overlapping data.\n",
    "\n",
    "##### k-means Clustering Performance\n",
    "\n",
    "This next example helps illustrate that clustering may not always perform well. We will use `taxon_df` for this demonstration. The dataset contains 120 observed plants from 4 different islands. Specific characteristics, represented as seven variables, for taxonomy were measured for all plants.\n",
    "\n",
    "A k-means algorithm is run on the data for 4 clusters. Run the code cell below to see the results. The data itself is sorted so that the first 30 observations belong to taxon I, the next 30 to taxon II, the next 30 to taxon III, etc. This cluster analysis was performed with 4 clusters; however, as you will see, the results are not very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920435f6-cf75-4910-8e21-aaf9efd881fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon_km = cls.KMeans(n_clusters=4).fit(taxon_df)\n",
    "\n",
    "taxon_km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd348d5-1f39-4722-afcc-c507de1b94cd",
   "metadata": {},
   "source": [
    "Look at the first 30 records; the observations are placed across different clusters. All of these observations should be just one single cluster. The next group of 30 exhibit a similar issue. Clustering did not perform very well. If we run the clustering algorithm again with just 3 clusters, instead of 4, the results come out cleaner. This won’t do, because we know this data contains 4 groups. There are other statistical techniques that may produce categorization on this data. \n",
    "\n",
    "[Read more about k-means clustering here](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html).\n",
    "\n",
    "We will now shift our discussion to some other clustering approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc21cea-6538-47c3-9611-14024a06c869",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering\n",
    "\n",
    "This type of clustering is based on the distance of objects to one another (i.e., dendrogram). A dendrogram is a diagram representing a tree.\n",
    "\n",
    "* Agglomerative: single elements merge into clusters\n",
    "  * Single Linkage - nearest neighbor\n",
    "  * Complete Linkage - similarity based on farthest data points\n",
    "* Divisive: start with dataset and divide into subsets\n",
    "  * Single Attribute - similarity is based on possessing similar values from those that differ\n",
    "  * All Attribute - like Single Attribute, except more than one attribute is taken into consideration\n",
    "\n",
    "##### Agglomerative Analysis\n",
    "\n",
    "We will now try our hand at a hierarchical clustering technique, agglomerative analysis, for categorization of the taxon data. We will also use the full taxonomy data we imported in notebook setup as `taxon_complete_df`. \n",
    "\n",
    "Agglomerative is an additive process. It starts out with each observation and slowly clusters them, based on Euclidean distance, until only one cluster remains. Run the code below to see an example dendrogram with the taxon data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15fcd4-60ae-4e95-844c-6a7b15af40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = hier.linkage(taxon_df, 'single')\n",
    "dn = hier.dendrogram(z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1919d1a-4441-4d7b-9fec-ad89ae35f6ae",
   "metadata": {},
   "source": [
    "Compared to the k-means clustering performed earlier, the results are not really improved. No matter which break we look at, the clusters involve observations from more than one group. Clustering did not appear to perform very well with this data. This is an important lesson to learn about clustering. Clustering may not find a global optimum; in many situations it only finds the local optimum. \n",
    "\n",
    "To help further illustrate this, let's generate a decision tree (we will elaborate on these later). Note that we are now using `taxon_complete_df`. Run the code cell below to generate and visualize the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43c3f5-b3bd-4e63-a4c6-44bdeef830c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use one-hot encode for categorical variables\n",
    "onehot_data = pd.get_dummies(taxon_complete_df[['Petals','Internode','Sepal','Bract','Petiole','Leaf','Fruit']], drop_first=True)\n",
    "\n",
    "# create a decision tree classifier instance\n",
    "tree_model = tree.DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "# fit the model\n",
    "tree_example = tree_model.fit(onehot_data, taxon_complete_df.Taxon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a2daa-43d7-4491-a647-63493e83da9e",
   "metadata": {},
   "source": [
    "We can use the built-in function from the `tree` library to plot the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060759a5-ded0-41a2-b2d8-e800e3e34376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the feature names\n",
    "tree_labels = list(onehot_data.columns.values)\n",
    "\n",
    "# define the class names\n",
    "tree_cn = list(taxon_complete_df.Taxon.unique())\n",
    "tree.plot_tree(tree_example, feature_names=tree_labels, class_names=tree_cn, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb796282-70be-4335-bf06-9cdc1767f97f",
   "metadata": {},
   "source": [
    "We can see that the tree was able to partition the data into four separate groups based on `Sepal`, `Leaf`, and `Petiole`. This is much cleaner than clustering. \n",
    "\n",
    "In various projects you engage in, it is often beneficial to attempt multiple types of statistical techniques. This process allows you to see where your analyses differ and agree. The temptation, typically, is to use a single process and then accept the results at face value. Don’t just settle for using one type of analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010aba4-1734-4149-b582-b13c9cecfc88",
   "metadata": {},
   "source": [
    "##### Another Agglomerative Analysis Example\n",
    "\n",
    "Okay, one more hierarchical clustering example. Again, we will apply agglomerative analysis. \n",
    "\n",
    "For this section of the tutorial, we will use `pg_df`. This data provides observations on plant growth for 54 plant species on 89 plots of land.\n",
    "\n",
    "We calculate the Euclidean distance for all of the rows and columns. Note that we will only do this for the first 54 columns; the other 5 (plot, lime, species, hay, pH) are not considered for this. Once calculated, the results can be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef16be3-e0af-48bc-9996-89cfb9721019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "agg_example = cls.AgglomerativeClustering(linkage='ward').fit(pg_df.iloc[:,0:54])\n",
    "agg_example.labels_\n",
    "\n",
    "# visualize results\n",
    "z = hier.linkage(pg_df, 'single')\n",
    "dn = hier.dendrogram(z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1172c27-5d5f-44b3-86f7-7f5894c10362",
   "metadata": {},
   "source": [
    "Observe the second break in the clustering (the second blue line from the left). The plants on the left of the break belong to land plots 11 and 14. The land plots 11 and 14 are high-nitrogen plots receiving phosphorus.\n",
    "\n",
    "We will not go into detail or cover examples on the next two clustering approaches. They are introduced so that you are aware of these clustering approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13bfd3-1a95-4855-8ab0-1d3bebab611e",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Models (GMM)\n",
    "\n",
    "Distribution-based models relying on the probability of a point being related to the centroid. Think of a three-dimensional bell-shaped curve. GMM utilizes variance and means to generate groupings. The `sklearn.mixture` package provides code for GMM implementation. [Check out the documentation for more information](https://scikit-learn.org/stable/modules/mixture.html).\n",
    "\n",
    "#### Density-based Clustering\n",
    "\n",
    "Data points are associated into clusters based on density criteria among a set of points; i.e. more dense areas are clustered. Unlike k-means clustering, density-based clustering is not restricted to elliptical shapes. This tends to provide a more organic clustering. A density-based clustering approach, the DBSCAN algorithm, is provided with scikit-learn. [Check out the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).\n",
    "\n",
    "## Classification\n",
    "\n",
    "These class of statistical techniques are some of the most popular. These are considered *supervised learning* techniques because they have target variables. The output variable is categorical (nominal or ordinal) in nature. The purpose of this family of techniques is to classify data based on a class label (your target variable). \n",
    "\n",
    "For example, we could have a dataset containing incoming internet traffic for a fictional business. The data contains many columns of information such as origination URL, origination datetime, content, size (in MB), and threat level. Threat level can be a binary value, `high` or `low` assigned by the firewall. Using classification, the variable `threat level` is the target. Using the other columns of data, the values in one are assigned to either the `low` or `high` value of `threat level`. How can we use this? we can program an automated script to use those values to determine if incoming traffic is a threat to our organization. If the traffic is labeled as `high`, then we block it.\n",
    "\n",
    "### Insurance Example\n",
    "\n",
    "To help illustrate how classification operates, let's consider another example. In this example, we are developing a model to help sift new automobile insurance customers into low or high risk status. High-risk customers are more expensive, and should be billed a higher premium. Assume we have the following data:\n",
    "\n",
    "| Age | Gender | Car Type | Risk |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| 20 | Male | SUV | High |\n",
    "| 18 | Female | Sport | High |\n",
    "| 40 | Male | Sport | High |\n",
    "| 50 | Male | Sedan | Low |\n",
    "| 33 | Female | Minivan | Low |\n",
    "| 31 | Female | Sedan | Low |\n",
    "| 29 | Male | Sedan | High |\n",
    "| 47 | Female | Truck | Low |\n",
    "\n",
    "The following classification tree is generated. Note, the most important factor in determining risk is age. Those who are 30-years of age or younger are automatically classified as *high risk*. The next criteria is the type of vehicle. In the image below, a `Sport` type of vehicle is automatically placed in the *high-risk* group. \n",
    "\n",
    "<div><center>\n",
    "<img src=\"assets/insurance_classification.png\" width=500>\n",
    "</center></div>\n",
    "\n",
    "This model is valuable to the insurance company. When new customers apply for insurance, the model can automatically assign them into a category and generate an estimate of cost. For example, assume a new set of customers apply for insurance.\n",
    "\n",
    "| Age | Car Type |\n",
    "|:---:|:---:|\n",
    "| 27 | Sport |\n",
    "| 34 | Minivan |\n",
    "| 55 | Truck |\n",
    "| 34 | Sport |\n",
    "\n",
    "The classification model will then assign the following risk classifications:\n",
    "\n",
    "| Age | Car Type | Risk |\n",
    "|:---:|:---:|:---:|\n",
    "| 27 | Sport | High |\n",
    "| 34 | Minivan | Low |\n",
    "| 55 | Truck | Low |\n",
    "| 34 | Sport | High |\n",
    "\n",
    "### Types of Classification and Accuracy\n",
    "\n",
    "Many different types of classification techniques exist. The following are just a few:\n",
    "\n",
    "* Decision tree techniques\n",
    "  * CART (Classification and Regression Tree)\n",
    "  * ID3/4/5 Algorithms and numerous variants\n",
    "* Statistical analysis \n",
    "  * Logistic Regression (presented in last week's tutorial)\n",
    "  * Bayesian classifiers\n",
    "* Neural networks\n",
    "* Support vector machines\n",
    "* Case-based reasoning/Genetic algorithms\n",
    "\n",
    "Not all classification models are built exactly the same. Many methods exist to determine the accuracy of the classifier. One such method is a confusion matrix. Simply put, a confusion matrix compares the predicted values of the data against the actual values of the data. In other words, how well did the classifier do its job?\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr><th></th><th></th><th colspan=\"2\">True Class</th></tr>\n",
    "<tr><th></th><th></th><th>Positive</th><th>Negative</th></tr>\n",
    "</thead> \n",
    "<tr><th style=\"border-top-style: hidden;\" rowspan=\"2\" scope=\"rowgroup\">Predicted Class</th>\n",
    "    <th style=\"border-top-style: hidden;\" scope=\"row\">Positive</th>\n",
    "    <td bgcolor=\"#90d696\"; align=\"center\">True Positive Count (TP)</td>\n",
    "    <td bgcolor=\"#ff9494\"; align=\"center\">False Positive Count (FP)</td>\n",
    "</tr>\n",
    "<tr><th scope=\"row\">Negative</th>\n",
    "    <td bgcolor=\"#ff9494\"; align=\"center\">False Negative Count (FN)</td>\n",
    "    <td bgcolor=\"#90d696\"; align=\"center\">True Negative Count (TN) </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79df12-1686-4816-8295-5fcb42cd77a1",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "A decision tree is a structure that can be used to divide up a large collection of records into successively smaller sets of records by applying a sequence of simple decision rules ([Linoff & Berry, 2011](https://dl.acm.org/doi/abs/10.5555/2543983)). Decision trees consist of a set of rules for dividing a large heterogeneous population into smaller and smaller homogeneous groups based on a target variable. The target variable is usually categorical. As you saw in our discussion on clustering, a decision tree can operate similarly to a cluster technique to reduce the number or records in a dataset.\n",
    "\n",
    "Decision trees are popular for both classification and thus prediction (Supervised/Directed). The attractiveness of this approach is due largely to the fact that decision trees represent rules expressed in both English and SQL. This makes interpretation straight-forward and simple. Also, they can be used for data exploration, becoming a powerful first step in model building. Although decision trees can be used to estimate continuous values, there are better ways. When used for estimation, the term should be *regression trees* instead of *decision trees*.\n",
    "\n",
    "How do decision trees work their magic? They recursively divide data until each division consists of examples from one class. These steps include\n",
    "1. Create a root node and assign all of the training data to it\n",
    "2. Select the best splitting attribute\n",
    "3. Add a branch to the root node for each value of the split. Split the data into mutually exclusive subsets along the lines of the specific split\n",
    "4. Repeat steps 2 and 3 for each and every leaf node until the stopping criteria is reached\n",
    "\n",
    "During the growth phase, the tree is built by recursively partitioning the data until each partition is either **pure** (contains members of the same class) or sufficiently small. The form of the split used to partition the data depends on the type of the attribute used in the split.\n",
    "* For a continuous attribute A, splits are of the form `value(A) < x` where `x` is a constant value in the domain of `A`.\n",
    "* For a categorical attribute A, splits are of the form `value(A) ∈ x` where `X` is in the domain of `A`\n",
    "* For a missing value; preferable to throwing out the record or imputing a value; consider null as its own branch\n",
    "\n",
    "While growing the tree, the goal at each node is to determine the split point that *best* divides the training records belonging to that leaf. A *full tree* is reached when it is not possible to do any more splits or to a predetermined depth. Note that full trees may not be best at classifying a set of new records. \n",
    "\n",
    "Depending on the type of predictor variables you are using for your target will determine which type of splitting occurs. In addition, continuous and categorical variables behave differently. For continuous variables, because numeric inputs are only used to compare their values at the split points, decision trees are not sensitive to outliers or skewed distributions. This can be valuable if your data contains lots of outliers and you are unable to remove them. \n",
    "\n",
    "If you are using categorical variables, the simplest approach is to split on each class (level) of the variable if you have more than two. However, this often yields poor results because high branching factors quickly reduce the population of training records available for lower nodes. An approach around this is to group the classes that, taken individually, predict similar outcomes.\n",
    "\n",
    "We will now go over some examples of regression trees and classification trees in Python. We will use `pollute_df_trees` and `taxon_complete_df` in this next section of the tutorial.\n",
    "\n",
    "### Regression Trees\n",
    "\n",
    "The tree function in Python comes from the library `sklearn` library. This is a class object you pass data into. scikit-learn provides a classification tree and a regression tree. The tree function uses the `Gini Index` as default, but you can switch to `entropy`.\n",
    "\n",
    "The classification tree and regression tree within the `sklearn` provides many options. Some of these options are useful for pruning the tree. Some of the more important ones are listed below:\n",
    "* `max_depth`: an integer representing the number of *levels* for nodes\n",
    "* `max_leaf_nodes`: the best nodes are selected when growing the tree until the number is reached; when used, `max_depth` is ignored\n",
    "* `min_samples_split`: the minimum number of samples each node requires for a split to occur\n",
    "* `min_samples_leaf`: the minimum number of samples required at a leaf node\n",
    "\n",
    "Using the pollution data, the following code is run to create the regression tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b4605-9ed6-476e-8f4b-6bbeb0843ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model_2 = tree.DecisionTreeRegressor().fit(pollute_df_trees, pollute_df.Pollution)\n",
    "\n",
    "col_names = list(pollute_df_trees.columns.values)\n",
    "\n",
    "tree.plot_tree(tree_model_2, feature_names=col_names, filled=True, rounded=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aee6ea-1151-4019-8021-523f81f71998",
   "metadata": {},
   "source": [
    "The output is too large to adequately display within this document. However, you can see that the output contains many nodes and leaves; it is not very useful. We could modify the figure size by increasing font size of `plot_tree()` and figure size of `plt.figure()` to make our figure more readable.\n",
    "\n",
    "```Python\n",
    "plt.figure(figsize=(10,8))  # set plot size (denoted in inches)\n",
    "tree.plot_tree(tree_model_2, feature_names=col_names,\n",
    "                filled=True, rounded=True,\n",
    "                fontsize=10)  # set font size\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Or, we can use the `graphviz` module which gives us more options for output format and size. Even without customizing the font and figure size, `graphviz` gives us a more readable figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1af58a-79a9-48cb-b88d-37f0e94281eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_mod_graph = export_graphviz(tree_model_2, filled=True, feature_names=col_names)\n",
    "graph = graphviz.Source(tree_mod_graph, format=\"png\")  # or \"svg\" for better resolution\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a1018-bf9c-4a5a-b380-fab47e26397c",
   "metadata": {},
   "source": [
    "This is a little easier to read (but still not great given the size of the tree). Take a look at the first split for `Industry`. The split value is at 748. The right side of the split, that with False, reduces the sample size per node past 5. For example, after `Population`, `Rain` has a sample of only 4 while the next split of `Population` has 3.\n",
    "\n",
    "Next we will try using the criteria `min_samples_split` and `min_samples_leaf` and set both to 5 in order to optimize our model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88955162-6539-4c83-a212-ba1569e6860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "tree_model_2 = tree.DecisionTreeRegressor(min_samples_split=5, min_samples_leaf=5)\n",
    "tree_model_2.fit(pollute_df_trees, pollute_df.Pollution)\n",
    "\n",
    "# visualize tree with graphviz\n",
    "tree_mod_graph = export_graphviz(tree_model_2, filled=True, feature_names=col_names)\n",
    "graph = graphviz.Source(tree_mod_graph, format=\"png\") # or \"svg\" for better resolution\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4223e-f058-46f3-bd83-84d71b6620f1",
   "metadata": {},
   "source": [
    "The results are much improved. The results of the tree are well plotted for a visual representation. The output reveals `Industry` is the first split and represents the most important variable for splitting.\n",
    "\n",
    "How did the algorithm determine `Industry` as the first split to represent the most important variable for splitting? To help explain the process of the tree operation, the following plot is created. On the *y*-axis is the target, `Pollution`; on the *x*-axis is `Industry`.\n",
    "<br>\n",
    "<div><center>\n",
    "<img src=\"assets/industry-pollution.png\" width=500>\n",
    "</center></div>  \n",
    "<br>\n",
    "\n",
    "Each independent variable is assessed by determining how much deviance in the dependent variable is explained. Deviance can be thought of as the measurement of explanatory power of a tree, like $R^2$ is for regression. In this case, `Industry` explains the most for `Pollution`. The deviance is based on a threshold value for each predictor; it produces two mean values: 1 above the threshold, and one below. The value 748 (the dashed, vertical line in the figure above) is the chosen threshold value of `Industry` based on deviance.\n",
    "\n",
    "The mean values for the two groups are shown via the two horizontal lines. Both of these mean values are used to calculate the deviance. The algorithm iterates through all *x*-values of `Industry` as the threshold (i.e. the vertical line); a mean below and above the threshold is calculated (i.e. the horizontal lines). For each threshold, the *x*-value with the lowest deviance is chosen. The data set is then split based on this threshold value. The program then runs through the other independent variables for each of the new data subsets. This continues until further deviance is not possible or there are too few data points (fewer than 6 cases is default).\n",
    "\n",
    "#### Cost Complexity\n",
    "\n",
    "An important question is, \"How do I determine how many nodes to prune from a tree?\" Does an objective method exist to help with pruning? We will go over one method that relies on the concept of cost complexity. As cost complexity increases, so too does the number of nodes pruned. Said a different way, as the number of nodes decrease, the more cost complexity increases.\n",
    "\n",
    "For this, we will create a training and testing subset from the original data. We use the method `cost_complexity_pruning_path()` to extract the alpha values and impurities associated with the minimal cost complexity path. At this time we are not interested in the impurities; only the alpha values. The alpha values are the cost complexity scores. Essentially, we are looking for the alphas (i.e., cost complexity scores) with the smallest value. The nodes with the smallest alphas are pruned first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11367a-a9c3-450f-b853-58229eaa2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pollute_df_trees, pollute_df.Pollution)\n",
    "path = tree_model_2.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b2ad6-4663-4db6-8866-d400ad8534e7",
   "metadata": {},
   "source": [
    "Now that we have a list of the alphas, we will train a tree using those alpha values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5859a-a83a-462c-aa25-87b5e79b2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = tree.DecisionTreeRegressor(ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "\n",
    "print(\n",
    "    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(clfs[-1].tree_.node_count, ccp_alphas[-1])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ef37c-4be0-4993-8d6d-d53e131293d3",
   "metadata": {},
   "source": [
    "We output the last value in the list `ccp_alphas` which will return the number of nodes and the alpha value. In a moment, we will confirm this in a plot. The last step is to generate two plots. The first plots the number of nodes against cost complexity. Look at the last data point in the first plot below. This confirms what we showedin the printed statement above. The second plot provides the depth of the tree (i.e., number of levels) against the cost complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7ca28-4db7-45bd-bc42-31448bff2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha (i.e., cost-complexity)\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha (i.e., cost-complexity)\")\n",
    "ax[1].set_ylabel(\"depth of tree\")\n",
    "ax[1].set_title(\"Depth vs alpha\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c46ce1-e5af-4c61-a324-c67810bc5054",
   "metadata": {},
   "source": [
    "We see in these plots that as alpha increases, the number of nodes and the number of levels of the tree decrease. This is an inverse relationship, which is expected for a tree. We can use this to inform decision making for pruning. Specifically, we can identify where the most drastic change for alpha occurs for both number of nodes and depth. This is where we can effectively reduce the size of the tree.\n",
    "\n",
    "### Classification Trees\n",
    "\n",
    "Recall that the `taxon_complete_df` data for the classification tree contains taxon data for various plant flora and fauna with the following variables:\n",
    "\n",
    "* Taxon\n",
    "* Petals\n",
    "* Internode\n",
    "* Sepal\n",
    "* Bract\n",
    "* Petiole\n",
    "* Leaf\n",
    "* Fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723de7c-8457-4cb2-8cc5-9062eba350ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels\n",
    "col_names = list(taxon_complete_df.columns.values)\n",
    "cls_names = list(taxon_complete_df.Taxon.unique())\n",
    "\n",
    "# use one-hot encode for categorical variables\n",
    "onehot_data = pd.get_dummies(taxon_df[['Petals', 'Internode', 'Sepal', 'Bract', 'Petiole', 'Leaf', 'Fruit']], drop_first=True)\n",
    "\n",
    "# fit model\n",
    "tree_model_2 = tree.DecisionTreeClassifier().fit(onehot_data, taxon_complete_df.Taxon)\n",
    "\n",
    "# visualize tree\n",
    "tree.plot_tree(tree_model_2, feature_names=col_names[1:8], class_names=cls_names, filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a911b-109d-48d2-89bb-b6e7ef02a876",
   "metadata": {},
   "source": [
    "The resultant tree is plotted above. It may help to have the data in front of you to compare. The first class, `Class IV`, is distinguished by the size of the `Sepal`; that is, a size greater than 3.53. With a size less than or equal to 3.53 and a leaf size greater than 2, `Class III` plants are distinguished.\n",
    "\n",
    "In addition to interpreting a decision tree, it is important to actually assess the quality of the tree. This can be done by viewing a report of the classification as well as a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ea10b-d0ac-4052-b00a-c464a25393aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted values\n",
    "predicted = tree_model_2.predict(onehot_data)\n",
    "\n",
    "# get model performance evaluation metrics\n",
    "print(metcs.classification_report(taxon_complete_df.Taxon, predicted))\n",
    "\n",
    "# get confusion matrix\n",
    "cm = metcs.confusion_matrix(taxon_complete_df.Taxon, predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9cf71-5068-431d-a47a-9fa2fdfe67a2",
   "metadata": {},
   "source": [
    "The precision of classifying each group is 100% accurate. Additionally, the confusion matrix reveals that to be the case. The columns represent the actual values, the rows the predicted values. Below is a graphical representation of the confusion matrix. This gradient-based confusion matrix is not easily discerned because the accuracy is 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aafe82-1695-47cd-b991-dfa602654c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(cm)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Actual Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.xticks([0,1,2,3], ['I','II','III','IV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d150304-a9be-4403-9a8f-3680b64ebc27",
   "metadata": {},
   "source": [
    "Let's go over one more example for implementation of a classification tree. \n",
    "\n",
    "In this section of the tutorial, we will build a classification tree using `titanic_df`. Note that because it has only categorical data, the values must be converted to numeric first. We will take care of that in the first code cell below. We will then build and analyze the classification tree in the subsequent code cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4747a3-159e-4c65-810d-e7ac9b7e869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get desired columns\n",
    "num_cols = pd.DataFrame(titanic_df[['Class', 'Sex', 'Age']])\n",
    "\n",
    "# rename the columns to keep them distinct from original DataFrame\n",
    "num_cols.rename(columns={'Class':'Class2', 'Sex':'Sex2', 'Age':'Age2'}, inplace=True)\n",
    "\n",
    "# obtain the values to be converted\n",
    "num_cols['Class2'].unique()\n",
    "num_cols['Sex2'].unique()\n",
    "num_cols['Age2'].unique()\n",
    "\n",
    "# in week 9 tutorial we used the following method to convert values to numerical:  \n",
    "# num_cols.Class2 = (num_cols.Class2.replace(['1st', '2nd', '3rd', 'Crew'], [0, 1, 2, 3]) \n",
    "#                   .infer_objects(copy=False))\n",
    "\n",
    "# however this produces a warning related to converting data types (deprecation of downcasting behavior)\n",
    "# we will use a different method based on discussion in https://github.com/pandas-dev/pandas/issues/57734\n",
    "\n",
    "# convert Class2 to numeric\n",
    "replace_dict = {'1st': '0', '2nd': '1', '3rd': '2', 'Crew': '3'}  # keys are str and so must be values\n",
    "num_cols.Class2 = num_cols.Class2.replace(replace_dict).astype(int)  # cast values to int\n",
    "\n",
    "# convert Sex2 to numeric\n",
    "replace_dict = {'Female': '0', 'Male': '1'}\n",
    "num_cols.Sex2 = num_cols.Sex2.replace(replace_dict).astype(int)\n",
    "\n",
    "# convert Age2 to numeric\n",
    "replace_dict = {'Child': '0', 'Adult': '1'}\n",
    "num_cols.Age2 = num_cols.Age2.replace(replace_dict).astype(int)\n",
    "\n",
    "# create new df with numeric columns\n",
    "titanic_df_2 = pd.concat([titanic_df, num_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc23b1e-8b67-46fe-8925-148f19270c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "col_names = list(titanic_df_2[['Class','Sex','Age']].values)\n",
    "classnames = list(titanic_df_2['Survived'].unique())\n",
    "\n",
    "# fit model\n",
    "tree_model_3 = tree.DecisionTreeClassifier(min_samples_split=5, min_samples_leaf=5).fit(titanic_df_2[['Class2','Sex2','Age2']], titanic_df_2['Survived'])\n",
    "\n",
    "# visualize tree\n",
    "plt.figure(figsize=(14,8))  # set plot size (denoted in inches)\n",
    "tree.plot_tree(tree_model_3,\n",
    "                feature_names=col_names,\n",
    "                class_names=classnames,\n",
    "                filled=True, rounded=True,\n",
    "                fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8636000-7a5c-4713-ba81-7f41120434e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted values\n",
    "predicted = tree_model_3.predict(titanic_df_2.iloc[:,4:7])\n",
    "\n",
    "# get performance metrics for model evaluation\n",
    "print(metcs.classification_report(titanic_df_2['Survived'], predicted))\n",
    "\n",
    "# create and display confusion matrix\n",
    "cm = metcs.confusion_matrix(titanic_df_2['Survived'], predicted)\n",
    "print(cm)\n",
    "\n",
    "# visualize confusion matrix\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Actual Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.xticks([0,1], ['Yes','No'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866e5e0-383d-4bc2-ad27-ba60a8e6d55c",
   "metadata": {},
   "source": [
    "This model's performance is not as great as the prior model we built with `taxon_complete_df`. Check out [this Towards Data Science article to learn more about performance metrics used to evaluate models](https://towardsdatascience.com/performance-metrics-confusion-matrix-precision-recall-and-f1-score-a8fe076a2262/). \n",
    "\n",
    "That wraps up our discussion of classification! \n",
    "\n",
    "---\n",
    "## Further Reading\n",
    "\n",
    "We have covered *a lot* of information in this tutorial. Please review it as needed. As with last week's content, we are limited in our ability to dive into statistical theory. Here is a set of free, open-source resources that you may find helpful in developing your statistics and Python expertise for analytics:\n",
    "\n",
    "* [Improving Your Statistical Inferences by Daniël Lakens](https://lakens.github.io/statistical_inferences/)\n",
    "* [Python Data Science Handbook by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/)\n",
    "* [Think Stats: Probability and Statistics for Programmers by Allen B. Downey](https://greenteapress.com/thinkstats/thinkstats.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "Originally, I planned to include a section on Artificial Neural Networks (ANN) in this tutorial. Given the complexity of these topics and approaches, I have decided to include the description and demos of ANN in an separate, *optional* tutorial that will be shared with the class during Week 10. Familiarizing yourself with the content in that tutorial is not required for this week's course assignments. However, it will be provided for those who are interested in learning about ANN and its implementation in Python. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
